{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccdaac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_html in /opt/anaconda3/lib/python3.13/site-packages (0.10.0)\n",
      "Requirement already satisfied: fake-useragent in /opt/anaconda3/lib/python3.13/site-packages (2.2.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from requests_html) (2.32.3)\n",
      "Requirement already satisfied: pyquery in /opt/anaconda3/lib/python3.13/site-packages (from requests_html) (2.0.1)\n",
      "Requirement already satisfied: parse in /opt/anaconda3/lib/python3.13/site-packages (from requests_html) (1.21.1)\n",
      "Requirement already satisfied: bs4 in /opt/anaconda3/lib/python3.13/site-packages (from requests_html) (0.0.2)\n",
      "Requirement already satisfied: w3lib in /opt/anaconda3/lib/python3.13/site-packages (from requests_html) (2.1.2)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /opt/anaconda3/lib/python3.13/site-packages (from requests_html) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et_xmlfile in /opt/anaconda3/lib/python3.13/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in /opt/anaconda3/lib/python3.13/site-packages (from pyppeteer>=0.0.14->requests_html) (2026.1.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /opt/anaconda3/lib/python3.13/site-packages (from pyppeteer>=0.0.14->requests_html) (8.5.0)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from pyppeteer>=0.0.14->requests_html) (11.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /opt/anaconda3/lib/python3.13/site-packages (from pyppeteer>=0.0.14->requests_html) (4.67.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /opt/anaconda3/lib/python3.13/site-packages (from pyppeteer>=0.0.14->requests_html) (1.26.20)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /opt/anaconda3/lib/python3.13/site-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.13/site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests_html) (4.12.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/lib/python3.13/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.13/site-packages (from bs4->requests_html) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4->bs4->requests_html) (2.5)\n",
      "Requirement already satisfied: lxml>=2.1 in /opt/anaconda3/lib/python3.13/site-packages (from pyquery->requests_html) (5.3.0)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from pyquery->requests_html) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->requests_html) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->requests_html) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests_html fake-useragent pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6aadf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported correctly\n",
      "Pandas version: {pd.__version__}\n"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported correctly\")\n",
    "print(\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca393f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml_html_clean in /opt/anaconda3/lib/python3.13/site-packages (0.4.3)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.13/site-packages (from lxml_html_clean) (5.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae873a",
   "metadata": {},
   "outputs": [],
   "source": "# requests_html: provides HTMLSession for HTTP requests with optional JavaScript rendering via Pyppeteer\n# fake_useragent: generates realistic browser User-Agent strings from a live database to reduce bot-detection risk\n# re: Python's built-in regex module used here for pattern matching within raw HTML text\n# json: built-in module for deserializing JSON strings into Python dicts/lists\n# pandas: data manipulation library; used to structure extracted stats into tabular form\n# os: standard file system interface (available for path operations if needed)\nfrom requests_html import HTMLSession\nfrom fake_useragent import UserAgent\nimport re\nimport json\nimport pandas as pd\nimport os\n\nprint(\"Libraries imported correctly\")\nprint(f\"Pandas version: {pd.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46367f93",
   "metadata": {},
   "outputs": [],
   "source": "# UserAgent() initializes the fake-useragent pool. ua.random returns a randomly selected\n# browser User-Agent string, rotating the client identity on each request to avoid\n# server-side bot fingerprinting based on a static or repeated User-Agent header.\nua = UserAgent()\n\n# HTMLSession wraps requests.Session and optionally drives a headless Chromium browser\n# via Pyppeteer for JavaScript-rendered pages. For Next.js sites that pre-render their\n# data server-side into the HTML, calling .render() is not required — the __NEXT_DATA__\n# script tag is present in the initial HTML response.\nsession = HTMLSession()\n\nurl = \"https://www.fotmob.com/matches/galatasaray-vs-juventus/2u4xwc#5161868\"\n\n# Accept headers mimic a real browser request to pass basic server-side bot filters.\n# Accept-Language is set to Spanish to match the locale, though the __NEXT_DATA__ payload\n# is not locale-dependent for statistical data.\nheaders = {\n    \"User-Agent\" : ua.random,\n    \"Accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\": \"es-ES,es;q=0.9\"\n}\n\nprint(f\"URL objetivo: {url}\")\nprint(f\"User agent: {headers}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c48cf9",
   "metadata": {},
   "outputs": [],
   "source": "# session.get() sends an HTTP GET request with the spoofed headers.\n# timeout=15 sets a 15-second hard limit to prevent the script from hanging on slow responses.\n# response.text returns the full HTTP body decoded as a UTF-8 string (the complete HTML page).\n# The HTML is persisted to disk so subsequent development iterations can work from the\n# cached file rather than issuing repeated network requests to the same URL.\ntry:\n    response = session.get(url, headers=headers, timeout=15)\n    if response.status_code == 200:\n        data = response.text\n        print(f\"HTML: {data}\")\n        filename = \"fotmob_html.txt\"\n        # Open in write mode with explicit UTF-8 encoding to handle non-ASCII characters\n        # (player names, accented characters in stadium/team names)\n        with open(filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(data)\n        print(f\"Saved on {filename}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18657e8f",
   "metadata": {},
   "outputs": [],
   "source": "# FotMob is a Next.js application. Next.js injects its full server-side rendered state\n# into a <script id=\"__NEXT_DATA__\"> tag as a JSON blob. This tag is always present\n# in the initial HTML and contains all data used to hydrate the React component tree,\n# including match stats, lineups, and event data — without needing to reverse-engineer\n# any XHR/fetch API endpoints.\n#\n# re.DOTALL makes '.' match newline characters, allowing the pattern to capture\n# JSON content that spans multiple lines.\n# re.IGNORECASE handles any casing variation in the script tag attributes.\npattern = r'<script id=\"__NEXT_DATA__\" type=\"application/json\">(.*?)</script>'\n\nmatch = re.search(pattern, data, re.DOTALL | re.IGNORECASE)\n\nif match:\n    # match.group(1) extracts the first capture group — the raw JSON string\n    # between the opening and closing script tags\n    json_str = match.group(1).strip()\n    print(\"__NEXT_DATA__ found\")\n    try:\n        # json.loads() deserializes the JSON string into a nested Python dict/list structure\n        json_data = json.loads(json_str)\n        print(json_data)\n        print(f'{list(json_data.keys())}')\n    except json.JSONDecodeError as e:\n        print(f\"Error: {e}\")\nelse:\n    print(\"Error: Block not found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22339275",
   "metadata": {},
   "outputs": [],
   "source": "# Navigate the nested JSON structure to the match statistics payload.\n# Path: json_data['props']['pageProps']['content'] is the root content object for the match page.\n# ['stats']['Periods']['All']['stats'] contains stats aggregated across the full 90+ minutes.\n# The 'Periods' key also contains 'FirstHalf' and 'SecondHalf' sub-objects for\n# half-level breakdowns, which can be used to identify second-half performance drops.\nbase = json_data['props'][\"pageProps\"][\"content\"]\nstats_all = base[\"stats\"][\"Periods\"][\"All\"][\"stats\"]\nprint(stats_all)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be9934",
   "metadata": {},
   "outputs": [],
   "source": "# Flatten the nested stats list into a row-per-stat tabular structure.\n# The raw JSON organizes stats as: sections > items > [home_value, away_value].\n# Section header rows carry stats: [None, None] and contain no data — they are skipped.\n# Each valid item is appended as a dict with:\n#   'section'  — the category name (e.g. \"Shots\", \"Expected goals (xG)\", \"Passes\")\n#   'stat'     — the specific metric name within that category\n#   'home'     — the home team's value (may be int, float, or formatted string like \"396 (79%)\")\n#   'away'     — the away team's value in the same format\nrows = []\nfor section in stats_all:\n    section_name = section.get(\"title\")\n    for item in section.get(\"stats\", []):\n        values = item.get(\"stats\")\n        # Skip rows that are section-level title headers with no data values\n        if values == [None, None]:\n            continue\n        # values[0] = home team stat, values[1] = away team stat\n        rows.append({\n            \"section\" : section_name,\n            \"stat\" : item.get(\"title\"),\n            \"home\": values[0] if isinstance(values, list) and len(values) > 0 else None,\n            \"away\": values[1] if isinstance(values, list) and len(values) > 0 else None\n        })\n    print(rows)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7554e",
   "metadata": {},
   "outputs": [],
   "source": "# pd.DataFrame(rows) converts the list of dicts into a structured DataFrame.\n# Each row maps to a single stat: section | metric name | home value | away value.\n# Note: 'home' and 'away' columns are mixed-type — some stats are integers (e.g. shots),\n# others are floats (xG), and others are pre-formatted strings (e.g. \"396 (79%)\").\n# For numerical analysis, string-format columns need parsing (e.g. split on space, cast to int/float).\nimport pandas as pd\n\nstats_df = pd.DataFrame(rows)\nstats_df.head(30)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b533d2b",
   "metadata": {},
   "outputs": [],
   "source": "# The 'shotmap' key within the content payload contains individual shot events\n# for both teams, each as a nested dict with fields for xG, coordinates, outcome,\n# player, and shot type.\n# pd.json_normalize() flattens nested dictionaries within each shot record into\n# top-level columns, producing one row per shot. Nested keys become dot-separated\n# column names (e.g. player.name becomes player_name after normalization).\nshots = base['shotmap']['shots']\n\nshots_df = pd.json_normalize(shots)\nshots_df.head(50)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "23f2c470",
   "metadata": {},
   "outputs": [],
   "source": "## Summary: FotMob Match Statistics Scraper\n\n### What This Notebook Does\n\nThis notebook extracts structured match statistics from FotMob using direct HTML scraping. FotMob is built on Next.js, which embeds its full server-side rendered state as a JSON blob inside a `<script id=\"__NEXT_DATA__\">` tag. This tag is present in the initial HTML response — no JavaScript execution or API reverse-engineering is required to access the data.\n\nThe pipeline is: HTTP GET with spoofed headers → regex extraction of `__NEXT_DATA__` → JSON deserialization → tree traversal → tabular flattening.\n\n### Key Techniques\n\n- **User-Agent rotation** (`fake_useragent`): Generates realistic browser strings to reduce automated-request detection. This is a surface-level measure; robust scraping at scale requires IP rotation, request throttling, and session management.\n- **`__NEXT_DATA__` extraction** (`re.search` with `DOTALL`): A reliable scraping pattern applicable to any Next.js-based site. The captured JSON contains the complete data model used to render the page.\n- **Nested JSON traversal**: The match stats are located at `props > pageProps > content > stats > Periods > All > stats`. The `Periods` object also exposes `FirstHalf` and `SecondHalf` keys for within-match breakdowns.\n- **`pd.json_normalize()`**: Flattens deeply nested shot event dicts into a flat DataFrame — critical when individual events contain nested player, team, and coordinate sub-objects.\n\n### Data Available\n\n| DataFrame | Content |\n|-----------|---------|\n| `stats_df` | Match-level stats across 8 categories (xG, shots, passes, physical metrics, duels, discipline) |\n| `shots_df` | Individual shot events with coordinates, xG, outcome, and player metadata |\n\n### Ideas to Extract More Value\n\n- **Multi-match aggregation**: Loop over multiple FotMob match URLs to build a season-level dataset. Persist to a database (SQLite, PostgreSQL) rather than in-memory DataFrames to handle scale.\n- **Half-level breakdown**: `Periods.FirstHalf` and `Periods.SecondHalf` expose per-half stats. Comparing xG and physical metrics between halves can reveal fatigue-driven performance drops.\n- **xG vs actual goals**: Accumulate xG across a season and compare to actual goals scored to quantify finishing efficiency (overperformance) or wasteful attacking play (underperformance).\n- **Physical metric tracking over a fixture schedule**: Sprint counts and total distances from `physical_metrics` can indicate squad fatigue during congested periods, useful for load management analysis.\n- **Shot location clustering**: Combine `shots_df` coordinates with xG to cluster shot origins and identify the zones each team most frequently attacks from and their quality distribution.\n- **Resilience at scale**: User-Agent spoofing is fragile under high request volume. For production pipelines, prefer an official data provider API or implement rate limiting, proxy rotation, and exponential backoff."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}